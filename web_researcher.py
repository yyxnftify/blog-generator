"""
Web Researcher ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰ãƒãƒƒãƒˆä¸Šã®é–¢é€£æƒ…å ±ã‚’è‡ªå‹•åé›†ã—ã€
è¨˜äº‹ç”Ÿæˆã®ã‚½ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã¨ã—ã¦æä¾›ã™ã‚‹ã€‚
"""

import requests
from bs4 import BeautifulSoup
import re
import time
import random

# ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆä¸€è¦§ï¼ˆãƒ–ãƒ­ãƒƒã‚¯å›é¿ç”¨ï¼‰
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15",
]


def get_headers():
    """ãƒ©ãƒ³ãƒ€ãƒ ãªãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ç”Ÿæˆ"""
    return {
        "User-Agent": random.choice(USER_AGENTS),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "ja,en-US;q=0.7,en;q=0.3",
    }


def search_google(keyword, num_results=8):
    """
    Googleæ¤œç´¢ã§ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®ä¸Šä½ãƒšãƒ¼ã‚¸URLã‚’å–å¾—ã™ã‚‹ã€‚
    â€»Googleæ¤œç´¢APIã®ä»£ã‚ã‚Šã«ç°¡æ˜“ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨ã€‚
    åˆ¶é™ãŒã‹ã‹ã‚‹å ´åˆã¯DuckDuckGoã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚
    """
    urls = []

    # ã¾ãšDuckDuckGoã§æ¤œç´¢ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ãŒç·©ã„ï¼‰
    urls = _search_duckduckgo(keyword, num_results)

    if not urls:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Googleæ¤œç´¢
        urls = _search_google_direct(keyword, num_results)

    return urls


def _search_duckduckgo(keyword, num_results=8):
    """DuckDuckGo HTMLã‹ã‚‰æ¤œç´¢çµæœã‚’å–å¾—"""
    try:
        url = f"https://html.duckduckgo.com/html/?q={requests.utils.quote(keyword)}"
        response = requests.get(url, headers=get_headers(), timeout=15)

        if response.status_code != 200:
            print(f"  DuckDuckGoæ¤œç´¢å¤±æ•—: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ {response.status_code}")
            return []

        soup = BeautifulSoup(response.text, "html.parser")
        results = []

        # DuckDuckGo HTMLç‰ˆã®çµæœãƒªãƒ³ã‚¯ã‚’å–å¾—
        for link in soup.find_all("a", class_="result__a"):
            href = link.get("href", "")
            # DuckDuckGoã®ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆURLã‹ã‚‰actualã®URLã‚’æŠ½å‡º
            if "uddg=" in href:
                import urllib.parse
                parsed = urllib.parse.parse_qs(urllib.parse.urlparse(href).query)
                if "uddg" in parsed:
                    actual_url = parsed["uddg"][0]
                    if _is_valid_url(actual_url):
                        results.append(actual_url)
            elif href.startswith("http") and _is_valid_url(href):
                results.append(href)

            if len(results) >= num_results:
                break

        print(f"  DuckDuckGoæ¤œç´¢: {len(results)}ä»¶ã®URLå–å¾—")
        return results

    except Exception as e:
        print(f"  DuckDuckGoæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def _search_google_direct(keyword, num_results=8):
    """Googleæ¤œç´¢ã‹ã‚‰çµæœURLã‚’å–å¾—ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
    try:
        url = f"https://www.google.co.jp/search?q={requests.utils.quote(keyword)}&hl=ja&num={num_results}"
        response = requests.get(url, headers=get_headers(), timeout=15)

        if response.status_code != 200:
            print(f"  Googleæ¤œç´¢å¤±æ•—: ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ {response.status_code}")
            return []

        soup = BeautifulSoup(response.text, "html.parser")
        results = []

        for a_tag in soup.find_all("a", href=True):
            href = a_tag["href"]
            if "/url?q=" in href:
                actual_url = href.split("/url?q=")[1].split("&")[0]
                if _is_valid_url(actual_url):
                    results.append(actual_url)

            if len(results) >= num_results:
                break

        print(f"  Googleæ¤œç´¢: {len(results)}ä»¶ã®URLå–å¾—")
        return results

    except Exception as e:
        print(f"  Googleæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
        return []


def _is_valid_url(url):
    """æœ‰åŠ¹ãªè¨˜äº‹URLã‹ãƒã‚§ãƒƒã‚¯ï¼ˆåºƒå‘Šã‚„SNSã‚’é™¤å¤–ï¼‰"""
    exclude_domains = [
        "google.", "youtube.", "twitter.", "facebook.",
        "instagram.", "amazon.", "rakuten.", "yahoo.",
        "pinterest.", "tiktok.", "linkedin.",
    ]
    for domain in exclude_domains:
        if domain in url.lower():
            return False
    return url.startswith("http")


def extract_page_content(url, max_chars=5000):
    """
    æŒ‡å®šURLã®ãƒšãƒ¼ã‚¸æœ¬æ–‡ã‚’æŠ½å‡ºã™ã‚‹ã€‚
    HTMLæ§‹é€ ã‹ã‚‰ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—ã—ã€ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹ã€‚
    """
    try:
        response = requests.get(url, headers=get_headers(), timeout=15)
        response.encoding = response.apparent_encoding

        if response.status_code != 200:
            return None

        soup = BeautifulSoup(response.text, "html.parser")

        # ä¸è¦ãªè¦ç´ ã‚’é™¤å»
        for tag in soup.find_all(["script", "style", "nav", "footer", "header", "aside", "form"]):
            tag.decompose()

        # ã‚¿ã‚¤ãƒˆãƒ«å–å¾—
        title = ""
        title_tag = soup.find("title")
        if title_tag:
            title = title_tag.get_text(strip=True)

        # ãƒ¡ã‚¿ãƒ‡ã‚£ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³å–å¾—
        meta_desc = ""
        meta_tag = soup.find("meta", attrs={"name": "description"})
        if meta_tag:
            meta_desc = meta_tag.get("content", "")

        # è¦‹å‡ºã—æ§‹é€ ã‚’å–å¾—
        headings = []
        for h_tag in soup.find_all(["h1", "h2", "h3"]):
            text = h_tag.get_text(strip=True)
            if text and len(text) > 2:
                headings.append(f"[{h_tag.name.upper()}] {text}")

        # æœ¬æ–‡ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        # articleã‚¿ã‚°å„ªå…ˆã€ãªã‘ã‚Œã°mainã€ãªã‘ã‚Œã°body
        content_area = soup.find("article") or soup.find("main") or soup.find("body")

        if not content_area:
            return None

        # ãƒ‘ãƒ©ã‚°ãƒ©ãƒ•ã‚’å„ªå…ˆçš„ã«å–å¾—
        paragraphs = content_area.find_all("p")
        if paragraphs:
            text_content = "\n".join([
                p.get_text(strip=True) for p in paragraphs
                if len(p.get_text(strip=True)) > 15  # çŸ­ã™ãã‚‹ã‚‚ã®ã¯é™¤å¤–
            ])
        else:
            text_content = content_area.get_text(separator="\n", strip=True)

        # ãƒ†ã‚­ã‚¹ãƒˆã®æ•´å½¢
        text_content = re.sub(r'\n{3,}', '\n\n', text_content)
        text_content = text_content[:max_chars]

        return {
            "url": url,
            "title": title,
            "meta_description": meta_desc,
            "headings": headings[:20],  # ä¸Šä½20è¦‹å‡ºã—ã¾ã§
            "content": text_content,
        }

    except Exception as e:
        print(f"  ãƒšãƒ¼ã‚¸å–å¾—ã‚¨ãƒ©ãƒ¼ ({url[:60]}...): {e}")
        return None


def research_keyword(keyword, max_sources=5):
    """
    ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«é–¢ã™ã‚‹Webæƒ…å ±ã‚’åŒ…æ‹¬çš„ã«åé›†ã™ã‚‹ã€‚
    è¤‡æ•°ã‚½ãƒ¼ã‚¹ã‹ã‚‰æƒ…å ±ã‚’é›†ã‚ã€è¨˜äº‹ç”Ÿæˆã«ä½¿ãˆã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™ã€‚

    Returns:
        dict: {
            "keyword": str,
            "sources": list[dict],  # å–å¾—ã—ãŸå„ãƒšãƒ¼ã‚¸ã®æƒ…å ±
            "combined_headings": list[str],  # å…¨ã‚½ãƒ¼ã‚¹ã®è¦‹å‡ºã—ä¸€è¦§
            "combined_content": str,  # å…¨ã‚½ãƒ¼ã‚¹ã®å†…å®¹ã‚’çµ±åˆ
            "source_count": int,
        }
    """
    print(f"\nğŸ” ãƒªã‚µãƒ¼ãƒé–‹å§‹: ã€Œ{keyword}ã€")

    # æ¤œç´¢å®Ÿè¡Œ
    urls = search_google(keyword, num_results=max_sources + 3)

    if not urls:
        print("  âš  æ¤œç´¢çµæœãŒå–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return {
            "keyword": keyword,
            "sources": [],
            "combined_headings": [],
            "combined_content": "",
            "source_count": 0,
        }

    # å„ãƒšãƒ¼ã‚¸ã®å†…å®¹ã‚’å–å¾—
    sources = []
    all_headings = []
    all_content_parts = []

    for i, url in enumerate(urls[:max_sources + 3]):
        if len(sources) >= max_sources:
            break

        print(f"  ğŸ“„ å–å¾—ä¸­ ({i+1}/{len(urls)}): {url[:80]}...")
        page_data = extract_page_content(url)

        if page_data and page_data["content"] and len(page_data["content"]) > 100:
            sources.append(page_data)
            all_headings.extend(page_data["headings"])
            all_content_parts.append(
                f"ã€å‡ºå…¸: {page_data['title'][:60]}ã€‘\n{page_data['content'][:3000]}"
            )

        # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ã®ãŸã‚å°‘ã—å¾…ã¤
        time.sleep(random.uniform(1.0, 2.5))

    # çµæœã‚’çµ±åˆ
    combined_content = "\n\n---\n\n".join(all_content_parts)

    # å†…å®¹ãŒå¤šã™ãã‚‹å ´åˆã¯ãƒˆãƒªãƒ 
    if len(combined_content) > 30000:
        combined_content = combined_content[:30000] + "\n...(ä»¥ä¸‹ç•¥)"

    result = {
        "keyword": keyword,
        "sources": sources,
        "combined_headings": list(set(all_headings)),  # é‡è¤‡é™¤å»
        "combined_content": combined_content,
        "source_count": len(sources),
    }

    print(f"  âœ… ãƒªã‚µãƒ¼ãƒå®Œäº†: {len(sources)}ä»¶ã®ã‚½ãƒ¼ã‚¹ã‚’å–å¾—")
    return result


def research_multiple_keywords(keywords, max_sources_per_keyword=3):
    """
    è¤‡æ•°ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒªã‚µãƒ¼ãƒã‚’å®Ÿè¡Œã—ã€çµæœã‚’çµ±åˆã™ã‚‹ã€‚
    ä¾‹: ["ãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ©ã‚¤ãƒ  è‚²ã¦æ–¹", "ãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ©ã‚¤ãƒ  å†¬è¶Šã—"]
    """
    all_results = []
    for kw in keywords:
        result = research_keyword(kw, max_sources=max_sources_per_keyword)
        all_results.append(result)
        time.sleep(2)  # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é–“ã®å¾…ã¡æ™‚é–“

    return all_results


# ãƒ†ã‚¹ãƒˆç”¨
if __name__ == "__main__":
    result = research_keyword("ãƒ•ã‚£ãƒ³ã‚¬ãƒ¼ãƒ©ã‚¤ãƒ  è‚²ã¦æ–¹", max_sources=3)
    print(f"\n=== ãƒªã‚µãƒ¼ãƒçµæœ ===")
    print(f"ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰: {result['keyword']}")
    print(f"ã‚½ãƒ¼ã‚¹æ•°: {result['source_count']}")
    for s in result['sources']:
        print(f"  - {s['title'][:50]}")
    print(f"è¦‹å‡ºã—æ•°: {len(result['combined_headings'])}")
    print(f"çµ±åˆãƒ†ã‚­ã‚¹ãƒˆé•·: {len(result['combined_content'])}æ–‡å­—")
